import ollama
import neo4j
from sentence_transformers import SentenceTransformer
# Assuming the previous code is saved as graph_rag_system.py
import graph_rag_system as grs
from bert_score import score as bert_score_calculate # Renamed to avoid conflict
import json
import os
import time
import logging
from dotenv import load_dotenv
import pandas as pd
from typing import List, Dict, Tuple, Optional, Any

# --- Configuration & Setup ---

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv() # Ensure .env file is in the same directory or accessible

# Use configurations defined in graph_rag_system or redefine if needed
NEO4J_URI = os.getenv("NEO4J_URI", "neo4j://localhost:7687")
NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
DEFAULT_DB = os.getenv("NEO4J_DATABASE", "neo4j")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3")

# --- Evaluation Data ---

# Use the same dummy text as defined in graph_rag_system.py
# You can replace this with reading from a file
DUMMY_TEXT = grs.DUMMY_TEXT # Or load from file
DOCUMENT_ID = "eval_doc_01" # Use a distinct ID for evaluation runs

# !! CRITICAL: Define your questions and reference answers based ONLY on DUMMY_TEXT !!
QA_PAIRS = [
    {
        "question": "What are the terrestrial planets?",
        "reference_answer": "The terrestrial planets are Mercury, Venus, Earth, and Mars. They are the four inner planets primarily composed of rock and metal."
    },
    {
        "question": "Describe the composition of Jupiter and Saturn.",
        "reference_answer": "Jupiter and Saturn are gas giants composed mainly of hydrogen and helium."
    },
    {
        "question": "What is the Kuiper Belt?",
        "reference_answer": "The Kuiper Belt is a region of small icy bodies, including dwarf planets like Pluto and Eris, located beyond Neptune."
    },
    {
        "question": "Why is Mars called the Red Planet?",
        "reference_answer": "Mars is called the Red Planet because the iron oxide prevalent on its surface gives it a reddish appearance."
    },
    {
        "question": "What makes Venus the hottest planet?",
        "reference_answer": "Venus is the hottest planet due to its thick, toxic atmosphere primarily composed of carbon dioxide, which leads to a runaway greenhouse effect."
    },
    # Add more diverse questions covering different aspects of the text
]

# --- Evaluation Metrics Functions ---

def calculate_bertscore(candidate: str, reference: str) -> Dict[str, List[float]]:
    """Calculates BERTScore between a candidate and reference answer."""
    if not candidate or not reference:
        logging.warning("BERTScore calculation skipped due to empty candidate or reference.")
        return {'precision': [0.0], 'recall': [0.0], 'f1': [0.0]}
    try:
        # Use a suitable model, ensure device placement is handled (e.g., 'cuda' if available)
        P, R, F1 = bert_score_calculate([candidate], [reference], lang="en", verbose=False, model_type='bert-base-uncased')
        # Return as dictionary matching expected structure, convert tensor to list
        return {'precision': P.tolist(), 'recall': R.tolist(), 'f1': F1.tolist()}
    except Exception as e:
        logging.error(f"Error calculating BERTScore: {e}")
        return {'precision': [0.0], 'recall': [0.0], 'f1': [0.0]} # Return default scores on error

def evaluate_with_llm(question: str, reference_answer: str, generated_answer: str, context: Optional[str] = None) -> Dict[str, Any]:
    """Uses LLM (Llama3) as a judge to evaluate the generated answer."""
    if not grs.ollama_client: # Check if ollama client is available
        logging.error("Ollama client not available for LLM evaluation.")
        return {"error": "Ollama client not available"}
    if not generated_answer:
        logging.warning("LLM evaluation skipped due to empty generated answer.")
        return {"error": "Empty generated answer"}

    # ** MODIFICATION: More structured LLM Judge Prompt **
    system_message = """You are an impartial evaluator assessing the quality of an answer generated by a RAG system based on a provided question and a reference answer. Evaluate the 'Generated Answer' based on the following criteria against the 'Reference Answer' and the 'Question':
    1.  **Faithfulness (0-5):** How accurately does the generated answer reflect the information present in the reference answer? Does it introduce information not found in the reference? (0=Completely inaccurate/hallucinated, 5=Perfectly faithful)
    2.  **Answer Relevance (0-5):** How well does the generated answer address the specific question asked? (0=Completely irrelevant, 5=Directly and fully answers the question)
    3.  **Coherence (0-5):** Is the generated answer fluent, grammatically correct, and easy to understand? (0=Incoherent, 5=Perfectly coherent)

    Output *only* a single JSON object with keys "faithfulness_score", "relevance_score", "coherence_score", and "reasoning" (a brief explanation for the scores). Example:
    {"faithfulness_score": 4, "relevance_score": 5, "coherence_score": 5, "reasoning": "The answer accurately reflects the reference and fully answers the question, with good coherence."}
    """

    # Context can optionally be included if the judge needs to verify against the original source
    prompt = f"""Evaluate the following:

    **Question:**
    {question}

    **Reference Answer:**
    {reference_answer}

    **Generated Answer:**
    {generated_answer}

    **Evaluation Criteria:**
    1. Faithfulness (0-5): Accuracy compared to Reference Answer.
    2. Answer Relevance (0-5): How well it answers the Question.
    3. Coherence (0-5): Fluency and grammar.

    Output your evaluation as a single JSON object with keys "faithfulness_score", "relevance_score", "coherence_score", and "reasoning":"""

    response = grs.call_ollama(prompt, system_message)

    try:
        # Attempt to parse the JSON response
        json_start = response.find('{')
        json_end = response.rfind('}') + 1
        if json_start != -1 and json_end != -1 and json_start < json_end:
            clean_response = response[json_start:json_end]
            eval_result = json.loads(clean_response)
            # Basic validation
            required_keys = ["faithfulness_score", "relevance_score", "coherence_score", "reasoning"]
            if all(key in eval_result for key in required_keys):
                return eval_result
            else:
                logging.error(f"LLM evaluation response missing required keys: {response}")
                return {"error": "LLM response missing keys", "raw_response": response}
        else:
            logging.error(f"Could not find valid JSON object in LLM evaluation response: {response}")
            return {"error": "LLM response not valid JSON", "raw_response": response}
    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse JSON from LLM evaluation response: {e}\nResponse: {response}")
        return {"error": f"JSON Parsing Error: {e}", "raw_response": response}
    except Exception as e:
        logging.error(f"Unexpected error during LLM evaluation parsing: {e}\nResponse: {response}")
        return {"error": f"Unexpected Parsing Error: {e}", "raw_response": response}


def get_triple_count(graph_id: str) -> int:
    """Counts the number of triples (relationships) for a given graph_id."""
    if not grs.neo4j_driver:
        return 0
    query = "MATCH ()-[r:RELATED_TO]->() WHERE r.graph_id = $graph_id RETURN count(r) as triple_count"
    result = grs.execute_cypher_query(query, {"graph_id": graph_id})
    return result[0]['triple_count'] if result else 0

# --- RAG System Implementations/Wrappers ---

def run_vanilla_rag(question: str, document_text: str, graph_id: Optional[str] = None) -> str:
    """Placeholder for a simple Vanilla RAG (Chunk -> Embed -> Retrieve -> Synthesize)."""
    logging.info(f"Running Vanilla RAG for question: {question[:50]}...")
    # 1. Chunk document (simple splitting for demo)
    chunks = [document_text[i:i+500] for i in range(0, len(document_text), 400)] # Overlapping chunks

    # 2. Embed chunks and query
    chunk_embeddings = grs.generate_embeddings(chunks)
    query_embedding = grs.generate_embeddings([question.lower().strip()])

    if not query_embedding or not chunk_embeddings:
        return "Error: Could not generate embeddings for Vanilla RAG."

    # 3. Simple similarity search (cosine similarity) - replace with actual vector DB in practice
    # This is inefficient for large scale but demonstrates the principle
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
    # Get top N chunks
    top_k = 3
    top_indices = similarities.argsort()[-top_k:][::-1]
    retrieved_context = "\n---\n".join([chunks[i] for i in top_indices])

    # 4. Synthesize answer
    system_message = "You are a helpful AI assistant. Answer the user's query based *only* on the following provided text context. If the context doesn't contain the answer, state that."
    prompt = f"""User Query: {question}

    Context:
    ---
    {retrieved_context}
    ---

    Based *only* on the context above, answer the user query:"""

    answer = grs.call_ollama(prompt, system_message)
    return answer

def run_graph_rag_clusters(question: str, document_text: str, graph_id: str) -> str:
    """Wrapper for the existing cluster-based Graph RAG."""
    logging.info(f"Running Cluster-based Graph RAG (graph_id: {graph_id}) for question: {question[:50]}...")
    # Uses the imported function directly
    return grs.query_graph_rag(question, graph_id)

def run_graph_rag_triples(question: str, document_text: str, graph_id: str) -> str:
    """Graph RAG variant using all triples as context."""
    logging.info(f"Running Triple-based Graph RAG (graph_id: {graph_id}) for question: {question[:50]}...")
    if not grs.neo4j_driver or not grs.ollama_client:
        return "Error: Required components not initialized."

    # 1. Retrieve ALL triples for the graph_id
    query = """
    MATCH (s:Entity)-[r:RELATED_TO]->(o:Entity)
    WHERE s.graph_id = $graph_id AND o.graph_id = $graph_id AND r.graph_id = $graph_id
    RETURN s.name as subject, r.type as predicate, o.name as object
    LIMIT 500 // Add a limit to prevent excessively large context
    """
    triples_result = grs.execute_cypher_query(query, {"graph_id": graph_id})

    if not triples_result:
        logging.warning(f"No triples found for graph_id '{graph_id}'. Falling back to LLM.")
        return grs.call_ollama(f"Answer the following question based on general knowledge: {question}")

    # Format triples as text context
    triples_context = "\n".join([f"({r['subject']})-[{r['predicate']}]->({r['object']})" for r in triples_result])

    # 2. Synthesize answer using LLM with triples context
    system_message = "You are a helpful AI assistant. Answer the user's query based *only* on the provided knowledge graph triples context. If the context doesn't contain the answer, state that."
    prompt = f"""User Query: {question}

    Knowledge Graph Triples Context:
    ---
    {triples_context}
    ---

    Based *only* on the context above, answer the user query:"""

    answer = grs.call_ollama(prompt, system_message)
    return answer

# --- Placeholders for External RAG Systems ---

def run_external_rag_1(question: str, document_text: str, graph_id: Optional[str] = None) -> str:
    logging.info("Running External RAG System 1 (Placeholder)...")
    # Replace with actual call to your first external RAG system
    # Example: external_rag1_client.query(question, context=document_text)
    time.sleep(0.5) # Simulate work
    return f"Placeholder answer from External RAG 1 for: {question}"

def run_external_rag_2(question: str, document_text: str, graph_id: Optional[str] = None) -> str:
    logging.info("Running External RAG System 2 (Placeholder)...")
    time.sleep(0.6) # Simulate work
    return f"Placeholder answer from External RAG 2 for: {question}"

def run_external_rag_3(question: str, document_text: str, graph_id: Optional[str] = None) -> str:
    logging.info("Running External RAG System 3 (Placeholder)...")
    time.sleep(0.4) # Simulate work
    return f"Placeholder answer from External RAG 3 for: {question}"

def run_external_rag_4(question: str, document_text: str, graph_id: Optional[str] = None) -> str:
    logging.info("Running External RAG System 4 (Placeholder)...")
    time.sleep(0.5) # Simulate work
    return f"Placeholder answer from External RAG 4 for: {question}"


# --- Main Evaluation Harness ---

if __name__ == "__main__":
    logging.info("Starting Evaluation Pipeline...")

    # --- Setup Phase ---
    # Run the main pipeline from graph_rag_system to build the graphs
    # This assumes the file path and document ID match what's needed
    # We capture the cluster/summary info, but don't strictly need it for this eval script
    # unless we want to evaluate summarization quality itself.
    logging.info(f"Running main Graph RAG pipeline for document: {DOCUMENT_ID}")
    build_start_time = time.time()
    # We need the graph IDs used internally by the pipeline
    initial_graph_id = f"{DOCUMENT_ID}_initial"
    enriched_graph_id = f"{DOCUMENT_ID}_enriched"
    # Run the pipeline (it performs cleanup, build, cluster, enrich, cluster)
    _, _, _, _ = grs.run_graph_rag_pipeline(grs.DUMMY_FILE_PATH, DOCUMENT_ID)
    build_end_time = time.time()
    logging.info(f"Graph build and enrichment pipeline finished in {build_end_time - build_start_time:.2f} seconds.")

    # Get triple counts after build
    num_triples_initial = get_triple_count(initial_graph_id)
    num_triples_enriched = get_triple_count(enriched_graph_id)
    logging.info(f"Initial graph ({initial_graph_id}) triple count: {num_triples_initial}")
    logging.info(f"Enriched graph ({enriched_graph_id}) triple count: {num_triples_enriched}")


    # --- Evaluation Phase ---
    # Define RAG systems to test
    rag_systems_to_evaluate = {
        "Vanilla RAG": run_vanilla_rag,
        "Graph RAG (Triples, Initial)": run_graph_rag_triples,
        "Graph RAG (Clusters, Initial)": run_graph_rag_clusters,
        "Graph RAG (Triples, Enriched)": run_graph_rag_triples,
        "Graph RAG (Clusters, Enriched)": run_graph_rag_clusters,
        "External RAG 1 (Placeholder)": run_external_rag_1,
        "External RAG 2 (Placeholder)": run_external_rag_2,
        "External RAG 3 (Placeholder)": run_external_rag_3,
        "External RAG 4 (Placeholder)": run_external_rag_4,
    }

    results = []

    logging.info(f"Starting evaluation across {len(QA_PAIRS)} questions and {len(rag_systems_to_evaluate)} RAG systems...")

    for i, qa_data in enumerate(QA_PAIRS):
        question = qa_data["question"]
        reference = qa_data["reference_answer"]
        logging.info(f"\n--- Evaluating Question {i+1}/{len(QA_PAIRS)}: {question} ---")

        for name, rag_func in rag_systems_to_evaluate.items():
            logging.info(f"Running system: {name}")
            start_time = time.time()

            # Determine graph_id based on system name convention
            current_graph_id = None
            if "Initial" in name:
                current_graph_id = initial_graph_id
            elif "Enriched" in name:
                current_graph_id = enriched_graph_id

            # Run the specific RAG function
            try:
                # Pass graph_id only if the function expects it
                if current_graph_id:
                    generated_answer = rag_func(question, DUMMY_TEXT, current_graph_id)
                else:
                    generated_answer = rag_func(question, DUMMY_TEXT) # For Vanilla/External
            except Exception as e:
                 logging.error(f"Error running RAG system {name}: {e}")
                 generated_answer = f"Error running system: {e}"

            end_time = time.time()
            query_time = end_time - start_time
            logging.info(f"  Query Time: {query_time:.2f}s")
            logging.info(f"  Generated Answer: {generated_answer[:100]}...") # Log snippet

            # Calculate metrics
            logging.info("  Calculating metrics...")
            bertscore_results = calculate_bertscore(generated_answer, reference)
            llm_eval_result = evaluate_with_llm(question, reference, generated_answer) # Context optional for judge

            # Determine triple count for the relevant graph
            current_num_triples = 0
            if current_graph_id == initial_graph_id:
                current_num_triples = num_triples_initial
            elif current_graph_id == enriched_graph_id:
                current_num_triples = num_triples_enriched
            # Add N/A or 0 for non-graph systems if desired

            results.append({
                "question_id": i + 1,
                "question": question,
                #"reference_answer": reference, # Optional to include in final df
                "rag_system": name,
                "generated_answer": generated_answer,
                "query_time": query_time,
                "bertscore_precision": bertscore_results['precision'][0] if bertscore_results else None,
                "bertscore_recall": bertscore_results['recall'][0] if bertscore_results else None,
                "bertscore_f1": bertscore_results['f1'][0] if bertscore_results else None,
                "llm_faithfulness": llm_eval_result.get("faithfulness_score", None),
                "llm_relevance": llm_eval_result.get("relevance_score", None),
                "llm_coherence": llm_eval_result.get("coherence_score", None),
                "llm_reasoning": llm_eval_result.get("reasoning", llm_eval_result.get("error", "N/A")), # Store reasoning or error
                "num_triples_in_graph": current_num_triples,
            })
            logging.info("  Metrics calculated.")
            time.sleep(1) # Small delay between evaluations

    # --- Results Presentation ---
    logging.info("\n--- Evaluation Complete ---")

    if results:
        df_results = pd.DataFrame(results)
        # Optional: Calculate average scores per system
        avg_scores = df_results.groupby('rag_system')[[
            'query_time', 'bertscore_f1', 'llm_faithfulness', 'llm_relevance', 'llm_coherence'
        ]].mean()

        print("\n--- Average Scores Per System ---")
        print(avg_scores.to_markdown(numalign="left", stralign="left"))

        # Optional: Display full results table
        print("\n--- Full Evaluation Results ---")
        # Adjust display options if needed
        pd.set_option('display.max_rows', None)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', 1000)
        pd.set_option('display.max_colwidth', 150) # Show more of the answer/reasoning
        print(df_results.to_markdown(index=False, numalign="left", stralign="left"))

        # Optional: Save results to CSV
        try:
            results_filename = "rag_evaluation_results.csv"
            df_results.to_csv(results_filename, index=False)
            logging.info(f"Full results saved to {results_filename}")
            avg_filename = "rag_evaluation_averages.csv"
            avg_scores.to_csv(avg_filename)
            logging.info(f"Average scores saved to {avg_filename}")
        except Exception as e:
            logging.error(f"Failed to save results to CSV: {e}")

    else:
        logging.warning("No results were generated during the evaluation.")

    # --- Cleanup ---
    if grs.neo4j_driver:
        grs.neo4j_driver.close()
        logging.info("Neo4j driver closed.")

    logging.info("Evaluation script finished.")

